#!/usr/bin/env python3
"""
éªŒè¯æ¢¯åº¦ç¬¦å·çš„æ­£ç¡®æ€§
"""

import torch
import torch.nn.functional as F
import numpy as np

def simple_gaussians_forward(xy, colors, sigma_val=1.0):
    """
    ç®€åŒ–çš„é«˜æ–¯å‰å‘ä¼ æ’­ï¼Œç”¨äºæ¢¯åº¦éªŒè¯
    """
    H, W = 8, 8  # å°å›¾åƒ
    C = 3  # RGB
    
    # åˆ›å»ºåƒç´ ç½‘æ ¼
    y_coords = torch.arange(H, dtype=torch.float32)
    x_coords = torch.arange(W, dtype=torch.float32)
    grid_y, grid_x = torch.meshgrid(y_coords, x_coords, indexing='ij')
    
    output = torch.zeros(C, H, W)
    
    for i, (center, color) in enumerate(zip(xy, colors)):
        dx = grid_x - center[0]
        dy = grid_y - center[1]
        sigma = sigma_val * (dx*dx + dy*dy)
        weight = torch.exp(-sigma)
        
        for c in range(C):
            output[c] += color[c] * weight
    
    return output

def test_gradient_direction():
    """æµ‹è¯•æ¢¯åº¦æ–¹å‘æ˜¯å¦æ­£ç¡®"""
    print("ğŸ”¬ æµ‹è¯•æ¢¯åº¦æ–¹å‘...")
    
    # ç®€å•è®¾ç½®ï¼š2ä¸ªé«˜æ–¯ï¼Œ8x8å›¾åƒ
    xy = torch.tensor([[2.0, 2.0], [4.0, 4.0]], requires_grad=True)
    colors = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], requires_grad=True)
    target = torch.ones(3, 8, 8) * 0.5  # ç›®æ ‡å›¾åƒï¼Œæ­£ç¡®å°ºå¯¸
    
    # å‰å‘ä¼ æ’­
    output = simple_gaussians_forward(xy, colors)
    
    # L1æŸå¤±
    loss = F.l1_loss(output, target)
    print(f"åˆå§‹æŸå¤±: {loss.item():.6f}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    print(f"xyæ¢¯åº¦: {xy.grad}")
    print(f"colorsæ¢¯åº¦: {colors.grad}")
    
    # éªŒè¯æ¢¯åº¦æ–¹å‘ï¼šæ‰‹åŠ¨å¾®è°ƒå‚æ•°ï¼Œçœ‹æŸå¤±æ˜¯å¦ä¸‹é™
    with torch.no_grad():
        lr = 0.01
        xy_new = xy - lr * xy.grad
        colors_new = colors - lr * colors.grad
        
        output_new = simple_gaussians_forward(xy_new, colors_new)
        loss_new = F.l1_loss(output_new, target)
        
        print(f"æ¢¯åº¦ä¸‹é™åæŸå¤±: {loss_new.item():.6f}")
        print(f"æŸå¤±å˜åŒ–: {loss_new.item() - loss.item():.6f} (åº”è¯¥ < 0)")
        
        return loss_new.item() < loss.item()

def test_inverse_scale_gradient():
    """æµ‹è¯•inverse scaleçš„æ¢¯åº¦å¤„ç†"""
    print("\nğŸ”¬ æµ‹è¯•inverse scaleæ¢¯åº¦...")
    
    # æµ‹è¯• y = 1/x çš„æ¢¯åº¦
    scale = torch.tensor([2.0, 3.0, 4.0], requires_grad=True)
    
    # å‰å‘: inverse_scale = 1/scale
    inverse_scale = 1.0 / scale
    
    # å‡è®¾æŸå¤±å¯¹inverse_scaleçš„æ¢¯åº¦
    v_inverse_scale = torch.tensor([0.1, -0.2, 0.3])
    
    # åå‘ä¼ æ’­
    inverse_scale.backward(v_inverse_scale)
    
    print(f"scaleæ¢¯åº¦ (PyTorch): {scale.grad}")
    
    # æ‰‹åŠ¨è®¡ç®—: d/d(scale) = d/d(1/scale) * d(1/scale)/d(scale) = v_inverse * (-1/scale^2)
    manual_grad = v_inverse_scale * (-1.0 / (scale.detach() ** 2))
    print(f"scaleæ¢¯åº¦ (æ‰‹åŠ¨): {manual_grad}")
    
    # éªŒè¯ä¸€è‡´æ€§
    diff = torch.abs(scale.grad - manual_grad).max()
    print(f"æ¢¯åº¦å·®å¼‚: {diff.item():.8f}")
    
    return diff < 1e-6

if __name__ == "__main__":
    print("ğŸ§ª æ¢¯åº¦ç¬¦å·å’Œæ–¹å‘éªŒè¯\n")
    
    try:
        result1 = test_gradient_direction()
        result2 = test_inverse_scale_gradient()
        
        if result1 and result2:
            print("\nâœ… æ¢¯åº¦ç¬¦å·éªŒè¯é€šè¿‡ï¼")
        else:
            print("\nâŒ æ¢¯åº¦ç¬¦å·æœ‰é—®é¢˜ï¼")
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()