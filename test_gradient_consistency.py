#!/usr/bin/env python3
"""
æµ‹è¯•æ‰‹å†™åå‘ä¼ æ’­ä¸PyTorch autogradçš„æ¢¯åº¦ä¸€è‡´æ€§
"""

import torch
import torch.nn.functional as F
from fused_ssim import fused_ssim

def test_l1_gradient():
    """æµ‹è¯•L1æŸå¤±çš„æ¢¯åº¦è®¡ç®—ä¸€è‡´æ€§"""
    print("ğŸ”¬ æµ‹è¯•L1æŸå¤±æ¢¯åº¦ä¸€è‡´æ€§...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    pred = torch.randn(3, 64, 64, requires_grad=True)
    target = torch.randn(3, 64, 64)
    
    # PyTorch autograd
    loss_auto = F.l1_loss(pred, target)
    loss_auto.backward()
    grad_auto = pred.grad.clone()
    
    # æ‰‹å†™æ¢¯åº¦
    with torch.no_grad():
        grad_manual = torch.sign(pred - target) / pred.numel()  # L1æ¢¯åº¦å½’ä¸€åŒ–
    
    # æ¯”è¾ƒ
    diff = torch.abs(grad_auto - grad_manual).mean()
    print(f"  L1æ¢¯åº¦å·®å¼‚: {diff.item():.8f}")
    
    return diff < 1e-6

def test_ssim_gradient():
    """æµ‹è¯•SSIMæŸå¤±çš„æ¢¯åº¦è®¡ç®—"""
    print("ğŸ”¬ æµ‹è¯•SSIMæŸå¤±æ¢¯åº¦...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    pred = torch.randn(3, 64, 64, requires_grad=True)
    target = torch.randn(3, 64, 64)
    
    # PyTorch autograd
    loss_auto = 1 - fused_ssim(pred.unsqueeze(0), target.unsqueeze(0))
    loss_auto.backward()
    grad_auto = pred.grad.clone()
    
    print(f"  SSIMæ¢¯åº¦èŒƒå›´: [{grad_auto.min().item():.6f}, {grad_auto.max().item():.6f}]")
    print(f"  SSIMæ¢¯åº¦å‡å€¼: {grad_auto.mean().item():.6f}")
    print(f"  SSIMæ¢¯åº¦æ ‡å‡†å·®: {grad_auto.std().item():.6f}")
    
    return True

def test_mixed_loss():
    """æµ‹è¯•æ··åˆæŸå¤±çš„æ¢¯åº¦"""
    print("ğŸ”¬ æµ‹è¯•L1+SSIMæ··åˆæŸå¤±...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    pred = torch.randn(3, 64, 64, requires_grad=True)
    target = torch.randn(3, 64, 64)
    
    l1_ratio = 1.0
    ssim_ratio = 0.1
    
    # PyTorch autograd
    l1_loss = l1_ratio * F.l1_loss(pred, target)
    ssim_loss = ssim_ratio * (1 - fused_ssim(pred.unsqueeze(0), target.unsqueeze(0)))
    total_loss = l1_loss + ssim_loss
    
    total_loss.backward()
    grad_auto = pred.grad.clone()
    
    print(f"  L1 Loss: {l1_loss.item():.6f}")
    print(f"  SSIM Loss: {ssim_loss.item():.6f}")
    print(f"  Total Loss: {total_loss.item():.6f}")
    print(f"  æ··åˆæ¢¯åº¦èŒƒå›´: [{grad_auto.min().item():.6f}, {grad_auto.max().item():.6f}]")
    print(f"  æ··åˆæ¢¯åº¦å‡å€¼: {grad_auto.mean().item():.6f}")
    
    return True

if __name__ == "__main__":
    print("ğŸ§ª æ¢¯åº¦ä¸€è‡´æ€§æµ‹è¯•\n")
    
    try:
        test_l1_gradient()
        test_ssim_gradient()
        test_mixed_loss()
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()